{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4637c0-f868-4a53-b252-21924831205b",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">What is Pytorch?</h1> \n",
    "\n",
    "    \n",
    "Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\r\n",
    "\r\n",
    "Tensorial library that uses the power of GPUs\r\n",
    "A deep learning research platform that provides maximum flexibility and speed\r\n",
    "Tensor is a term for the generalization of vectors (a quantity that has a magnitude and direction, for example, 5 meters to the left) to multiple directions/dimensions (5 meters to the left and to the right). The number of directions is referred to as the rank of the tensor. If that is confusing, just think of them as matrices. Matrix multiplication pops up a lot in machine learning and so having a library that can implement matrices and perform matrix multiplication very fast is very nice to have...i.e. PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "40f93c17-c967-4abb-9006-0c93e05770b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6269e+19, 7.0065e-45, 0.0000e+00],\n",
       "        [1.4013e-45, 1.3434e-21, 3.0837e-41]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x= torch.empty(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "59715955-f5f8-4b8c-a782-8be37b9582c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6926, 0.8677, 0.6430],\n",
       "        [0.0356, 0.5421, 0.4181]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.rand(2,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b12edaaa-fdfb-4fa2-bdcc-d128170d96e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4311, 0.3839, 0.5122],\n",
       "        [0.6417, 0.3807, 0.6953]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=torch.rand(2,3)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9bf52fcd-c4ca-4677-b735-6b2914b461b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also specify the data type\n",
    "m=torch.ones(2,3, dtype=torch.float64)\n",
    "m.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e9db11f3-544e-41e1-b2a3-001a045fe742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1237, 1.2516, 1.1552],\n",
       "        [0.6773, 0.9228, 1.1134]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y+z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d1804ef2-0120-435c-977d-3e1f0a461394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1237, 1.2516, 1.1552],\n",
       "        [0.6773, 0.9228, 1.1134]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "912013d4-1ef1-423b-a272-e9a365a07991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3744, 2.3891, 2.4122],\n",
       "        [2.0069, 2.3241, 1.5685]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "96368573-8a3e-4107-adad-6168113d9123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5649, 0.9584, 1.4722],\n",
       "        [1.9860, 0.6574, 1.0572]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba57bdf4-5ce3-4080-92e3-a57ffccd684d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5649, 0.9584, 1.4722],\n",
       "        [1.9860, 0.6574, 1.0572]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e54b16eb-564b-4551-96ee-79833173d365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9610, 1.7090, 1.1337],\n",
       "        [0.0210, 1.9368, 0.4010]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "aaedf86d-1c5f-49e5-a06f-105dda9e566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5620, 0.2868, 0.8868],\n",
      "        [0.6812, 0.0260, 0.0513],\n",
      "        [0.3546, 0.8236, 0.3618],\n",
      "        [0.6191, 0.8358, 0.4818],\n",
      "        [0.4800, 0.9501, 0.8236]])\n",
      "tensor([[0.2868, 0.8868],\n",
      "        [0.0260, 0.0513],\n",
      "        [0.8236, 0.3618],\n",
      "        [0.8358, 0.4818],\n",
      "        [0.9501, 0.8236]])\n",
      "tensor([[0.5620],\n",
      "        [0.6812],\n",
      "        [0.3546],\n",
      "        [0.6191],\n",
      "        [0.4800]])\n"
     ]
    }
   ],
   "source": [
    "# Lets print all the rows but one column and similarly all the column with 1 row\n",
    "k=torch.rand(5,3)\n",
    "print(k)\n",
    "print(k[:,1:])\n",
    "print(k[:,[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31581c30-39ca-4e44-86fd-25d18f2c91dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0445365309715271"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the actual value of the particular row and column we can use .item()\n",
    "k[0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "02a3e94c-689b-48fe-8e29-b131e3445f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert a tensor into numpy array\n",
    "import numpy as np\n",
    "a= torch.ones(1,5)\n",
    "b= a.numpy()\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bbc1d3ee-3a96-4d38-854f-f311b79c709d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d1bb3b81-dff1-4428-9808-c531a3d98bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert a numpy array to a tensor\n",
    "c=np.ones(5)\n",
    "d=torch.from_numpy(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ac9324c8-2444-4df0-94a5-c915de6c3d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sometimes when the tensor is defined there is an argument requires_grad=True, by default it is false, it helps in optimization later on\n",
    "e=torch.ones(3,4, requires_grad=True)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808756d-c952-4f1f-bc54-1887f5f4743d",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5733\">This section will talk about the autograd and how we can calculate gradients from it. Gradient is important for optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4d4c2182-a54e-46ff-9d1b-686651327473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6793, -0.3925, -0.0202], requires_grad=True)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=torch.randn(3, requires_grad=True)\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab1007-5b6b-4e79-9bdd-cbfb69ce244d",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The requires_grad=True argument tells PyTorch that we want to compute gradients with respect to this tensor during backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ed27b164-7008-4893-9d62-b317157073a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6793, 1.6075, 1.9798], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1=x1+2\n",
    "y1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fd0af-8320-4a89-bc49-b1ff47fad5eb",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">grad_fn tells you that y1 was created by an addition operation, and PyTorch has recorded this operation as part of the computation graph enabling PyTorch to efficiently compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ab57b407-e6b2-4822-ba3b-84008f15c84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.6404e+00, 3.0815e-01, 8.1969e-04], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2=x1*x1*2\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0ea0c9a4-132b-41c9-aa59-2dda623d8bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9831, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3= y2.mean()\n",
    "y3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727b099-dfc9-4380-9bc9-acaedcf04091",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">To calculate the gradient, all we need to now do is y3.backward() which will do dy3/dx1.. For scalar , no argument is needed but for vector we need to put the argument as same size as x1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ec956d73-13b5-4abb-bf64-c94002aa6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y3.backward() # for scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "05a55db3-5aae-4b46-badf-0a4e121d9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "v= torch.tensor([0.1,0.02,0.003], dtype=torch.float32)\n",
    "y2.backward(v) #for vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8a4e8a4d-886b-4732-9c8b-62d4207d99c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.7174e-01, -3.1402e-02, -2.4293e-04])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c0c3a-8170-478a-90e3-561b9e6b55d4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Let's say now that we don't want the requires_grad=true, so that pytorch wont track the history in computational graph: \n",
    "We essentially have three options: \n",
    "1) x1.requires_grad_(False), remember that whenever there is underscore _ it will modify the variable in place\n",
    "2) x.detach()\n",
    "3) with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ea2cb373-6ffe-460b-bb86-ab688c3fe63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6793, -0.3925, -0.0202])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.requires_grad_(False)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d6c23d53-06e3-4751-b612-0070778e0415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.6404e+00, 3.0815e-01, 8.1969e-04])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73806a-7d3b-4ba4-9752-14a5ad80e014",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Let's look at the trainning iteration where we want to make sure that for each iteration the x1.grad computes the same value. It is done by setting the grad value to zero after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "947a5209-d974-4e3a-8ee9-11098f52df90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights=torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    model_output=(weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "98ec7d2a-5a8e-4fcd-a82c-cd460eb397c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights=torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    model_output=(weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ca09e-2c74-4572-b471-c9d2c3ab2634",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4212f9f7-af1d-412c-ba0a-d95427a0571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.tensor(1.0)\n",
    "y=torch.tensor(2.0)\n",
    "\n",
    "w=torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#Forward pass and  compute the loss\n",
    "y_hat=w*x\n",
    "loss=(y_hat-y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "#Backward pass\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "## update weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658e836-0c3c-4075-85cd-51df3b2bfd51",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> For this part we will do everything manually only using numpy array and then translate the idea to pytorch ... We will start with our prediction and then compute gradient and then the loss and finally update the parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "747afc5f-597f-4b40-8a87-6ad8657694c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5)=0.000\n",
      "epoch1: w=1.200, loss= 30.00000000\n",
      "epoch3: w=1.872, loss= 0.76800019\n",
      "epoch5: w=1.980, loss= 0.01966083\n",
      "epoch7: w=1.997, loss= 0.00050331\n",
      "epoch9: w=1.999, loss= 0.00001288\n",
      "epoch11: w=2.000, loss= 0.00000033\n",
      "epoch13: w=2.000, loss= 0.00000001\n",
      "epoch15: w=2.000, loss= 0.00000000\n",
      "epoch17: w=2.000, loss= 0.00000000\n",
      "epoch19: w=2.000, loss= 0.00000000\n",
      "epoch21: w=2.000, loss= 0.00000000\n",
      "epoch23: w=2.000, loss= 0.00000000\n",
      "epoch25: w=2.000, loss= 0.00000000\n",
      "epoch27: w=2.000, loss= 0.00000000\n",
      "epoch29: w=2.000, loss= 0.00000000\n",
      "Prediction after training : f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# f=w*x\n",
    "X=np.array([1,2,3,4], dtype=np.float32)\n",
    "Y=np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w=0.0\n",
    "\n",
    "#model prediction\n",
    "def forward(x): \n",
    "    return w*x\n",
    "\n",
    "#loss =MSE\n",
    "\n",
    "def loss(y,y_predicted):\n",
    "    return((y_predicted-y)**2).mean()\n",
    "\n",
    "#gradient : \n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5)={forward(5):.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters=30\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction=forward pass \n",
    "    y_pred=forward(X)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y,y_pred)\n",
    "\n",
    "    #gradient\n",
    "    dw=gradient(X,Y,y_pred)\n",
    "\n",
    "    #update weights\n",
    "    w-=learning_rate*dw\n",
    "    if epoch %2==0:\n",
    "        print(f'epoch{epoch+1}: w={w:.3f}, loss= {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training : f(5) = {forward(5):.3f}')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cde3fd-8a9d-475e-9567-aea68a81b243",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Let's see how this translates to the pytorch framework by computing gradient using backward leaving the loss computation and forward pass as manually for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4e2acb59-9212-4ba2-b83e-4b37b2dadada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5)=0.000\n",
      "epoch1: w=0.300, loss= 30.00000000\n",
      "epoch3: w=0.772, loss= 15.66018772\n",
      "epoch5: w=1.113, loss= 8.17471695\n",
      "epoch7: w=1.359, loss= 4.26725292\n",
      "epoch9: w=1.537, loss= 2.22753215\n",
      "epoch11: w=1.665, loss= 1.16278565\n",
      "epoch13: w=1.758, loss= 0.60698116\n",
      "epoch15: w=1.825, loss= 0.31684780\n",
      "epoch17: w=1.874, loss= 0.16539653\n",
      "epoch19: w=1.909, loss= 0.08633806\n",
      "epoch21: w=1.934, loss= 0.04506890\n",
      "epoch23: w=1.952, loss= 0.02352631\n",
      "epoch25: w=1.966, loss= 0.01228084\n",
      "epoch27: w=1.975, loss= 0.00641066\n",
      "epoch29: w=1.982, loss= 0.00334642\n",
      "epoch31: w=1.987, loss= 0.00174685\n",
      "epoch33: w=1.991, loss= 0.00091188\n",
      "epoch35: w=1.993, loss= 0.00047601\n",
      "epoch37: w=1.995, loss= 0.00024848\n",
      "epoch39: w=1.996, loss= 0.00012971\n",
      "epoch41: w=1.997, loss= 0.00006770\n",
      "epoch43: w=1.998, loss= 0.00003534\n",
      "epoch45: w=1.999, loss= 0.00001845\n",
      "epoch47: w=1.999, loss= 0.00000963\n",
      "epoch49: w=1.999, loss= 0.00000503\n",
      "epoch51: w=1.999, loss= 0.00000262\n",
      "epoch53: w=2.000, loss= 0.00000137\n",
      "epoch55: w=2.000, loss= 0.00000071\n",
      "epoch57: w=2.000, loss= 0.00000037\n",
      "epoch59: w=2.000, loss= 0.00000019\n",
      "epoch61: w=2.000, loss= 0.00000010\n",
      "epoch63: w=2.000, loss= 0.00000005\n",
      "epoch65: w=2.000, loss= 0.00000003\n",
      "epoch67: w=2.000, loss= 0.00000001\n",
      "epoch69: w=2.000, loss= 0.00000001\n",
      "epoch71: w=2.000, loss= 0.00000000\n",
      "epoch73: w=2.000, loss= 0.00000000\n",
      "epoch75: w=2.000, loss= 0.00000000\n",
      "epoch77: w=2.000, loss= 0.00000000\n",
      "epoch79: w=2.000, loss= 0.00000000\n",
      "epoch81: w=2.000, loss= 0.00000000\n",
      "epoch83: w=2.000, loss= 0.00000000\n",
      "epoch85: w=2.000, loss= 0.00000000\n",
      "epoch87: w=2.000, loss= 0.00000000\n",
      "epoch89: w=2.000, loss= 0.00000000\n",
      "epoch91: w=2.000, loss= 0.00000000\n",
      "epoch93: w=2.000, loss= 0.00000000\n",
      "epoch95: w=2.000, loss= 0.00000000\n",
      "epoch97: w=2.000, loss= 0.00000000\n",
      "epoch99: w=2.000, loss= 0.00000000\n",
      "Prediction after training : f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# f=w*x\n",
    "X=torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y=torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#model prediction\n",
    "def forward(x): \n",
    "    return w*x\n",
    "\n",
    "#loss =MSE\n",
    "\n",
    "def loss(y,y_predicted):\n",
    "    return((y_predicted-y)**2).mean()\n",
    "\n",
    "#gradient : \n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5)={forward(5):.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters=100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction=forward pass \n",
    "    y_pred=forward(X)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y,y_pred)\n",
    "\n",
    "    #gradient= Backward pass\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w-=learning_rate*w.grad\n",
    "\n",
    "    #zero gradient\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch %2==0:\n",
    "        print(f'epoch{epoch+1}: w={w:.3f}, loss= {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training : f(5) = {forward(5):.3f}')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4472a1-f55a-4d9f-88bc-9c5874b1c295",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">As we can see that this method is not as efficient as doing it manually "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623d93b-89ac-4000-a073-a961288bf4de",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Now lets encorporate the loss and forward pass using pytorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "72df8f5a-1880-4048-a2c0-e3a78e75b066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5)=-0.995\n",
      "epoch1: w=-0.042, loss= 33.00695801\n",
      "epoch3: w=0.439, loss= 16.06250000\n",
      "epoch5: w=0.773, loss= 7.90224981\n",
      "epoch7: w=1.006, loss= 3.97134089\n",
      "epoch9: w=1.168, loss= 2.07674885\n",
      "epoch11: w=1.282, loss= 1.16259944\n",
      "epoch13: w=1.361, loss= 0.72052658\n",
      "epoch15: w=1.417, loss= 0.50576591\n",
      "epoch17: w=1.457, loss= 0.40047130\n",
      "epoch19: w=1.485, loss= 0.34790421\n",
      "epoch21: w=1.506, loss= 0.32074594\n",
      "epoch23: w=1.521, loss= 0.30584341\n",
      "epoch25: w=1.532, loss= 0.29686293\n",
      "epoch27: w=1.541, loss= 0.29075563\n",
      "epoch29: w=1.548, loss= 0.28605267\n",
      "epoch31: w=1.553, loss= 0.28204709\n",
      "epoch33: w=1.558, loss= 0.27839777\n",
      "epoch35: w=1.562, loss= 0.27494073\n",
      "epoch37: w=1.565, loss= 0.27159661\n",
      "epoch39: w=1.569, loss= 0.26832667\n",
      "epoch41: w=1.572, loss= 0.26511210\n",
      "epoch43: w=1.575, loss= 0.26194423\n",
      "epoch45: w=1.577, loss= 0.25881782\n",
      "epoch47: w=1.580, loss= 0.25573036\n",
      "epoch49: w=1.583, loss= 0.25268072\n",
      "epoch51: w=1.585, loss= 0.24966781\n",
      "epoch53: w=1.588, loss= 0.24669106\n",
      "epoch55: w=1.590, loss= 0.24375001\n",
      "epoch57: w=1.593, loss= 0.24084404\n",
      "epoch59: w=1.595, loss= 0.23797265\n",
      "epoch61: w=1.598, loss= 0.23513544\n",
      "epoch63: w=1.600, loss= 0.23233235\n",
      "epoch65: w=1.602, loss= 0.22956236\n",
      "epoch67: w=1.605, loss= 0.22682559\n",
      "epoch69: w=1.607, loss= 0.22412147\n",
      "epoch71: w=1.610, loss= 0.22144938\n",
      "epoch73: w=1.612, loss= 0.21880934\n",
      "epoch75: w=1.614, loss= 0.21620083\n",
      "epoch77: w=1.616, loss= 0.21362317\n",
      "epoch79: w=1.619, loss= 0.21107642\n",
      "epoch81: w=1.621, loss= 0.20856000\n",
      "epoch83: w=1.623, loss= 0.20607345\n",
      "epoch85: w=1.626, loss= 0.20361677\n",
      "epoch87: w=1.628, loss= 0.20118919\n",
      "epoch89: w=1.630, loss= 0.19879065\n",
      "epoch91: w=1.632, loss= 0.19642077\n",
      "epoch93: w=1.634, loss= 0.19407901\n",
      "epoch95: w=1.637, loss= 0.19176519\n",
      "epoch97: w=1.639, loss= 0.18947901\n",
      "epoch99: w=1.641, loss= 0.18721992\n",
      "Prediction after training : f(5) = 9.263\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn # neural network module\n",
    "# f=w*x\n",
    "X=torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y=torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test=torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size=n_features\n",
    "output_size=n_features\n",
    "model=nn.Linear(input_size, output_size)\n",
    "\n",
    "#loss =MSE\n",
    "#gradient : \n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5)={model(X_test).item():.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters=100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(n_iters):\n",
    "    #prediction=forward pass \n",
    "    y_pred=model(X)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y,y_pred)\n",
    "\n",
    "    #gradient= Backward pass\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch %2==0:\n",
    "        [w,b]=model.parameters()\n",
    "        print(f'epoch{epoch+1}: w={w[0][0]:.3f}, loss= {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training : f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877dd040-aedf-4581-a62c-6f3faee9265c",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Here we have used the model from the module itself, but we can customize it as follow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "130fc6ab-12c5-48fe-8ebb-880d295cff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5)=1.315\n",
      "epoch1: w=0.687, loss= 25.37613678\n",
      "epoch3: w=1.108, loss= 12.22059631\n",
      "epoch5: w=1.399, loss= 5.88659477\n",
      "epoch7: w=1.601, loss= 2.83694196\n",
      "epoch9: w=1.742, loss= 1.36860013\n",
      "epoch11: w=1.839, loss= 0.66160798\n",
      "epoch13: w=1.906, loss= 0.32118303\n",
      "epoch15: w=1.953, loss= 0.15724745\n",
      "epoch17: w=1.985, loss= 0.07828671\n",
      "epoch19: w=2.007, loss= 0.04023900\n",
      "epoch21: w=2.023, loss= 0.02188968\n",
      "epoch23: w=2.033, loss= 0.01302525\n",
      "epoch25: w=2.040, loss= 0.00872762\n",
      "epoch27: w=2.045, loss= 0.00662920\n",
      "epoch29: w=2.049, loss= 0.00558994\n",
      "epoch31: w=2.051, loss= 0.00506097\n",
      "epoch33: w=2.052, loss= 0.00477805\n",
      "epoch35: w=2.053, loss= 0.00461391\n",
      "epoch37: w=2.053, loss= 0.00450730\n",
      "epoch39: w=2.054, loss= 0.00442873\n",
      "epoch41: w=2.054, loss= 0.00436397\n",
      "epoch43: w=2.054, loss= 0.00430619\n",
      "epoch45: w=2.054, loss= 0.00425208\n",
      "epoch47: w=2.053, loss= 0.00420005\n",
      "epoch49: w=2.053, loss= 0.00414933\n",
      "epoch51: w=2.053, loss= 0.00409954\n",
      "epoch53: w=2.053, loss= 0.00405053\n",
      "epoch55: w=2.052, loss= 0.00400217\n",
      "epoch57: w=2.052, loss= 0.00395441\n",
      "epoch59: w=2.052, loss= 0.00390724\n",
      "epoch61: w=2.052, loss= 0.00386067\n",
      "epoch63: w=2.051, loss= 0.00381462\n",
      "epoch65: w=2.051, loss= 0.00376916\n",
      "epoch67: w=2.051, loss= 0.00372421\n",
      "epoch69: w=2.050, loss= 0.00367980\n",
      "epoch71: w=2.050, loss= 0.00363595\n",
      "epoch73: w=2.050, loss= 0.00359259\n",
      "epoch75: w=2.049, loss= 0.00354977\n",
      "epoch77: w=2.049, loss= 0.00350744\n",
      "epoch79: w=2.049, loss= 0.00346563\n",
      "epoch81: w=2.049, loss= 0.00342430\n",
      "epoch83: w=2.048, loss= 0.00338349\n",
      "epoch85: w=2.048, loss= 0.00334314\n",
      "epoch87: w=2.048, loss= 0.00330328\n",
      "epoch89: w=2.047, loss= 0.00326391\n",
      "epoch91: w=2.047, loss= 0.00322502\n",
      "epoch93: w=2.047, loss= 0.00318656\n",
      "epoch95: w=2.047, loss= 0.00314857\n",
      "epoch97: w=2.046, loss= 0.00311104\n",
      "epoch99: w=2.046, loss= 0.00307393\n",
      "Prediction after training : f(5) = 10.094\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn # neural network module\n",
    "# f=w*x\n",
    "X=torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y=torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test=torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size=n_features\n",
    "output_size=n_features\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "#model=nn.Linear(input_size, output_size)\n",
    "model=LinearRegression(input_size, output_size)\n",
    "#loss =MSE\n",
    "#gradient : \n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5)={model(X_test).item():.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters=100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(n_iters):\n",
    "    #prediction=forward pass \n",
    "    y_pred=model(X)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y,y_pred)\n",
    "\n",
    "    #gradient= Backward pass\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch %2==0:\n",
    "        [w,b]=model.parameters()\n",
    "        print(f'epoch{epoch+1}: w={w[0][0]:.3f}, loss= {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training : f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e82d43-5712-4f63-bf4c-6be9622e53e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Coffea-Santosh]",
   "language": "python",
   "name": "conda-env-Coffea-Santosh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
